# Databricks Asset Bundles Monorepo Implementation Plan

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** Build a single-root Databricks Asset Bundle monorepo that scales across many domains/sources, with Python-defined resources auto-generated via `just` commands and Pydantic validation.

**Architecture:** Monorepo with one `databricks.yml` at root. Each domain/source pair is a Python package under `src/<domain>/<source>/` with DLT transformations across four layers (raw, base, staging, final). Resource definitions live in `resources/<domain>/<source>/` and are generated by `just` commands backed by Pydantic models. Targets: dev, staging, prod — one workspace per environment.

**Tech Stack:** Databricks Asset Bundles (pydabs), Python 3.10+, uv, Pydantic, just, pytest, DLT (`pyspark.pipelines`), PySpark

**Reference template:** `/tmp/databricks-pydabs-template` — the official pydabs template. Do not deviate significantly from its patterns.

**Namespace mapping:** `catalog = domain`, `schema = source`

---

## Task 1: Root Bundle Configuration

**Files:**
- Create: `databricks.yml`

**Step 1: Create `databricks.yml`**

This is the single root bundle config. It follows the pydabs template pattern exactly — minimal YAML with Python resource loading, variables, and three targets.

```yaml
# This is a Databricks asset bundle definition for data-platform.
# See https://docs.databricks.com/dev-tools/bundles/index.html for documentation.
bundle:
  name: data-platform

python:
  venv_path: .venv
  # Functions called to load resources defined in Python. See resources/__init__.py
  resources:
    - "resources:load_resources"

include:
  - resources/*.yml
  - resources/**/*.yml

artifacts:
  python_artifact:
    type: whl
    build: uv build --wheel

# Variable declarations. These variables are assigned per target below.
variables:
  catalog:
    description: The Unity Catalog catalog to use (maps to domain)
  schema:
    description: The Unity Catalog schema to use (maps to source)

targets:
  dev:
    mode: development
    default: true
    workspace:
      host: https://dev.cloud.databricks.com  # TODO: replace with actual dev workspace URL
    variables:
      catalog: dev_${bundle.name}
      schema: ${workspace.current_user.short_name}

  staging:
    workspace:
      host: https://staging.cloud.databricks.com  # TODO: replace with actual staging workspace URL
      root_path: /Shared/.bundle/${bundle.name}/${bundle.target}
    variables:
      catalog: staging_${bundle.name}
      schema: default

  prod:
    mode: production
    workspace:
      host: https://prod.cloud.databricks.com  # TODO: replace with actual prod workspace URL
      root_path: /Shared/.bundle/${bundle.name}/${bundle.target}
    variables:
      catalog: ${bundle.name}
      schema: default
    permissions:
      - user_name: deploy@company.com  # TODO: replace with actual deploy identity
        level: CAN_MANAGE
```

**Step 2: Verify YAML is valid**

Run: `python -c "import yaml; yaml.safe_load(open('databricks.yml'))"`
Expected: No output (valid YAML)

**Step 3: Commit**

```bash
git add databricks.yml
git commit -m "feat: add root databricks.yml bundle configuration"
```

---

## Task 2: Python Project Configuration

**Files:**
- Create: `pyproject.toml`

**Step 1: Create `pyproject.toml`**

Follows the pydabs template pattern. Adds Pydantic for resource generation.

```toml
[project]
name = "data-platform"
version = "0.0.1"
requires-python = ">=3.10,<3.13"
dependencies = []

[dependency-groups]
dev = [
    "pytest",
    "pydantic>=2.0",
    "databricks-dlt",
    "databricks-connect>=15.4,<15.5",
    "databricks-bundles>=0.279.0",
    "pyyaml",
]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.hatch.build.targets.wheel]
packages = ["src"]

[tool.pytest.ini_options]
testpaths = ["tests"]

[tool.ruff]
line-length = 125
```

**Step 2: Commit**

```bash
git add pyproject.toml
git commit -m "feat: add pyproject.toml with project dependencies"
```

---

## Task 3: VS Code Configuration

**Files:**
- Create: `.vscode/settings.json`
- Create: `.vscode/extensions.json`
- Modify: `.gitignore` (append Databricks-specific entries)

**Step 1: Create `.vscode/settings.json`**

Copy from pydabs template with paths adjusted for monorepo structure.

```json
{
    "jupyter.interactiveWindow.cellMarker.codeRegex": "^# COMMAND ----------|^# Databricks notebook source|^(#\\s*%%|#\\s*\\<codecell\\>|#\\s*In\\[\\d*?\\]|#\\s*In\\[ \\])",
    "jupyter.interactiveWindow.cellMarker.default": "# COMMAND ----------",
    "python.testing.pytestArgs": ["."],
    "files.exclude": {
        "**/*.egg-info": true,
        "**/__pycache__": true,
        ".pytest_cache": true,
        "dist": true
    },
    "files.associations": {
        "**/.gitkeep": "markdown"
    },
    "python.analysis.typeCheckingMode": "basic",
    "python.analysis.extraPaths": ["src", "resources"],
    "python.analysis.diagnosticMode": "workspace",
    "python.analysis.stubPath": ".vscode",
    "python.defaultInterpreterPath": "./.venv/bin/python",
    "python.testing.unittestEnabled": false,
    "python.testing.pytestEnabled": true,
    "[python]": {
        "editor.defaultFormatter": "charliermarsh.ruff",
        "editor.formatOnSave": true
    }
}
```

**Step 2: Create `.vscode/extensions.json`**

```json
{
    "recommendations": [
        "databricks.databricks",
        "redhat.vscode-yaml",
        "charliermarsh.ruff"
    ]
}
```

**Step 3: Append Databricks entries to `.gitignore`**

Add the following block to the end of the existing `.gitignore`:

```
# Databricks
.databricks/
scratch/**
!scratch/README.md
**/explorations/**
!**/explorations/README.md
```

**Step 4: Commit**

```bash
git add .vscode/settings.json .vscode/extensions.json .gitignore
git commit -m "feat: add VS Code settings and update .gitignore for Databricks"
```

---

## Task 4: Resource Loader and Package Init

**Files:**
- Create: `resources/__init__.py`

**Step 1: Create `resources/__init__.py`**

This is the entry point referenced in `databricks.yml` under `python.resources`. It follows the pydabs template exactly.

```python
from databricks.bundles.core import (
    Bundle,
    Resources,
    load_resources_from_current_package_module,
)


def load_resources(bundle: Bundle) -> Resources:
    """
    'load_resources' function is referenced in databricks.yml and is responsible for loading
    bundle resources defined in Python code. This function is called by Databricks CLI during
    bundle deployment. After deployment, this function is not used.
    """

    # the default implementation loads all Python files in 'resources' directory
    return load_resources_from_current_package_module()
```

**Step 2: Commit**

```bash
git add resources/__init__.py
git commit -m "feat: add resource loader entry point"
```

---

## Task 5: Pydantic Models for Resource Generation

These models enforce type checking and validation when generating job and pipeline resource files via `just` commands. They are not used at runtime by Databricks — they produce Python files that Databricks reads.

**Files:**
- Create: `scripts/__init__.py` (empty)
- Create: `scripts/models.py`

**Step 1: Create `scripts/__init__.py`**

```python
"""Scripts for generating Databricks Asset Bundle resources."""
```

**Step 2: Create `scripts/models.py`**

```python
"""Pydantic models for validating resource generation inputs."""

from pydantic import BaseModel, field_validator
import re


class PipelineResourceConfig(BaseModel):
    """Configuration for generating a DLT pipeline resource file."""

    domain: str
    source: str
    name: str
    serverless: bool = True

    @field_validator("domain", "source", "name")
    @classmethod
    def must_be_snake_case(cls, v: str) -> str:
        if not re.match(r"^[a-z][a-z0-9_]*$", v):
            raise ValueError(f"'{v}' must be snake_case (lowercase, underscores, start with letter)")
        return v

    @property
    def resource_var_name(self) -> str:
        return f"{self.domain}_{self.source}_{self.name}_pipeline"

    @property
    def display_name(self) -> str:
        return f"{self.domain}_{self.source}_{self.name}_pipeline"

    @property
    def root_path(self) -> str:
        return f"src/{self.domain}/{self.source}"


class JobResourceConfig(BaseModel):
    """Configuration for generating a job resource file."""

    domain: str
    source: str
    name: str
    schedule_interval: int = 1
    schedule_unit: str = "DAYS"

    @field_validator("domain", "source", "name")
    @classmethod
    def must_be_snake_case(cls, v: str) -> str:
        if not re.match(r"^[a-z][a-z0-9_]*$", v):
            raise ValueError(f"'{v}' must be snake_case (lowercase, underscores, start with letter)")
        return v

    @field_validator("schedule_unit")
    @classmethod
    def valid_schedule_unit(cls, v: str) -> str:
        allowed = {"HOURS", "DAYS", "WEEKS"}
        if v not in allowed:
            raise ValueError(f"schedule_unit must be one of {allowed}")
        return v

    @property
    def resource_var_name(self) -> str:
        return f"{self.domain}_{self.source}_{self.name}_job"

    @property
    def display_name(self) -> str:
        return f"{self.domain}_{self.source}_{self.name}_job"

    @property
    def pipeline_resource_var(self) -> str:
        return f"{self.domain}_{self.source}_{self.name}_pipeline"
```

**Step 3: Commit**

```bash
git add scripts/__init__.py scripts/models.py
git commit -m "feat: add Pydantic models for resource generation validation"
```

---

## Task 6: Pipeline Resource Generator Script

**Files:**
- Create: `scripts/build_pipeline_resource.py`

**Step 1: Create `scripts/build_pipeline_resource.py`**

This script generates a Python resource file following the pydabs template's `Pipeline.from_dict()` pattern. It takes arguments, validates them with Pydantic, then writes the file to `resources/<domain>/<source>/`.

```python
"""Generate a DLT pipeline resource file for a domain/source."""

import argparse
import sys
from pathlib import Path

from scripts.models import PipelineResourceConfig


TEMPLATE = '''from databricks.bundles.pipelines import Pipeline

"""
DLT pipeline for {domain}/{source} - {name}.
"""

{resource_var_name} = Pipeline.from_dict(
    {{
        "name": "{display_name}",
        "catalog": "${{var.catalog}}",
        "schema": "${{var.schema}}",
        "serverless": {serverless},
        "root_path": "{root_path}",
        "libraries": [
            {{
                "glob": {{
                    "include": "{root_path}/transformations/**",
                }},
            }},
        ],
        "environment": {{
            "dependencies": [
                "--editable ${{workspace.file_path}}",
            ],
        }},
    }}
)
'''


def main() -> None:
    parser = argparse.ArgumentParser(description="Generate a DLT pipeline resource file")
    parser.add_argument("--domain", required=True, help="Domain name (snake_case)")
    parser.add_argument("--source", required=True, help="Source name (snake_case)")
    parser.add_argument("--name", required=True, help="Pipeline name (snake_case)")
    parser.add_argument("--serverless", default="true", help="Enable serverless (true/false)")
    args = parser.parse_args()

    try:
        config = PipelineResourceConfig(
            domain=args.domain,
            source=args.source,
            name=args.name,
            serverless=args.serverless.lower() == "true",
        )
    except Exception as e:
        print(f"Validation error: {e}", file=sys.stderr)
        sys.exit(1)

    output_dir = Path("resources") / config.domain / config.source
    output_dir.mkdir(parents=True, exist_ok=True)

    output_file = output_dir / f"{config.name}_pipeline.py"
    if output_file.exists():
        print(f"Error: {output_file} already exists", file=sys.stderr)
        sys.exit(1)

    content = TEMPLATE.format(
        domain=config.domain,
        source=config.source,
        name=config.name,
        resource_var_name=config.resource_var_name,
        display_name=config.display_name,
        serverless=config.serverless,
        root_path=config.root_path,
    )

    output_file.write_text(content)
    print(f"Created {output_file}")


if __name__ == "__main__":
    main()
```

**Step 2: Commit**

```bash
git add scripts/build_pipeline_resource.py
git commit -m "feat: add pipeline resource generator script"
```

---

## Task 7: Job Resource Generator Script

**Files:**
- Create: `scripts/build_job_resource.py`

**Step 1: Create `scripts/build_job_resource.py`**

This script generates a Python resource file following the pydabs template's `Job.from_dict()` pattern.

```python
"""Generate a job resource file for a domain/source."""

import argparse
import sys
from pathlib import Path

from scripts.models import JobResourceConfig


TEMPLATE = '''from databricks.bundles.jobs import Job

"""
Job for {domain}/{source} - {name}.
"""

{resource_var_name} = Job.from_dict(
    {{
        "name": "{display_name}",
        "trigger": {{
            "periodic": {{
                "interval": {schedule_interval},
                "unit": "{schedule_unit}",
            }},
        }},
        "parameters": [
            {{
                "name": "catalog",
                "default": "${{var.catalog}}",
            }},
            {{
                "name": "schema",
                "default": "${{var.schema}}",
            }},
        ],
        "tasks": [
            {{
                "task_key": "refresh_pipeline",
                "pipeline_task": {{
                    "pipeline_id": "${{resources.pipelines.{pipeline_resource_var}.id}}",
                }},
            }},
        ],
    }}
)
'''


def main() -> None:
    parser = argparse.ArgumentParser(description="Generate a job resource file")
    parser.add_argument("--domain", required=True, help="Domain name (snake_case)")
    parser.add_argument("--source", required=True, help="Source name (snake_case)")
    parser.add_argument("--name", required=True, help="Job name (snake_case)")
    parser.add_argument("--schedule-interval", type=int, default=1, help="Schedule interval (default: 1)")
    parser.add_argument("--schedule-unit", default="DAYS", help="Schedule unit: HOURS, DAYS, WEEKS (default: DAYS)")
    args = parser.parse_args()

    try:
        config = JobResourceConfig(
            domain=args.domain,
            source=args.source,
            name=args.name,
            schedule_interval=args.schedule_interval,
            schedule_unit=args.schedule_unit,
        )
    except Exception as e:
        print(f"Validation error: {e}", file=sys.stderr)
        sys.exit(1)

    output_dir = Path("resources") / config.domain / config.source
    output_dir.mkdir(parents=True, exist_ok=True)

    output_file = output_dir / f"{config.name}_job.py"
    if output_file.exists():
        print(f"Error: {output_file} already exists", file=sys.stderr)
        sys.exit(1)

    content = TEMPLATE.format(
        domain=config.domain,
        source=config.source,
        name=config.name,
        resource_var_name=config.resource_var_name,
        display_name=config.display_name,
        schedule_interval=config.schedule_interval,
        schedule_unit=config.schedule_unit,
        pipeline_resource_var=config.pipeline_resource_var,
    )

    output_file.write_text(content)
    print(f"Created {output_file}")


if __name__ == "__main__":
    main()
```

**Step 2: Commit**

```bash
git add scripts/build_job_resource.py
git commit -m "feat: add job resource generator script"
```

---

## Task 8: Justfile with Build Commands

**Files:**
- Create: `justfile`

**Step 1: Create `justfile`**

```just
# Data Platform - Databricks Asset Bundles

# Generate a DLT pipeline resource file
build-pipeline-resource domain source name serverless="true":
    uv run python -m scripts.build_pipeline_resource --domain {{domain}} --source {{source}} --name {{name}} --serverless {{serverless}}

# Generate a job resource file
build-job-resource domain source name schedule_interval="1" schedule_unit="DAYS":
    uv run python -m scripts.build_job_resource --domain {{domain}} --source {{source}} --name {{name}} --schedule-interval {{schedule_interval}} --schedule-unit {{schedule_unit}}

# Scaffold a new domain/source with directory structure
scaffold domain source:
    mkdir -p src/{{domain}}/{{source}}/transformations
    mkdir -p resources/{{domain}}/{{source}}
    mkdir -p tests/{{domain}}/{{source}}
    mkdir -p fixtures/{{domain}}/{{source}}
    @# src init files
    touch src/{{domain}}/__init__.py
    touch src/{{domain}}/{{source}}/__init__.py
    @# transformations gitkeep
    touch src/{{domain}}/{{source}}/transformations/.gitkeep
    @# resources init for auto-discovery
    touch resources/{{domain}}/__init__.py
    touch resources/{{domain}}/{{source}}/__init__.py
    @# tests init
    touch tests/{{domain}}/__init__.py
    touch tests/{{domain}}/{{source}}/__init__.py
    @# fixtures gitkeep
    touch fixtures/{{domain}}/{{source}}/.gitkeep
    @echo "Scaffolded {{domain}}/{{source}}"

# Deploy bundle to a target
deploy target="dev":
    databricks bundle deploy --target {{target}}

# Run a specific resource
run resource target="dev":
    databricks bundle run {{resource}} --target {{target}}

# Validate bundle configuration
validate target="dev":
    databricks bundle validate --target {{target}}

# Run tests
test *args:
    uv run pytest {{args}}
```

**Step 2: Commit**

```bash
git add justfile
git commit -m "feat: add justfile with resource generation and bundle commands"
```

---

## Task 9: Test Configuration (conftest.py and fixtures)

**Files:**
- Create: `tests/__init__.py` (empty)
- Create: `tests/conftest.py`
- Create: `fixtures/.gitkeep`

**Step 1: Create `tests/__init__.py`**

Empty file.

**Step 2: Create `tests/conftest.py`**

Follows the pydabs template pattern exactly.

```python
"""This file configures pytest.

This file is in the root since it can be used for tests in any place in this
project, including tests under resources/.
"""

import os
import sys
import pathlib
from contextlib import contextmanager


try:
    from databricks.connect import DatabricksSession
    from databricks.sdk import WorkspaceClient
    from pyspark.sql import SparkSession
    import pytest
    import json
    import csv
except ImportError:
    raise ImportError(
        "Test dependencies not found.\n\n"
        "Run tests using 'uv run pytest'. See http://docs.astral.sh/uv to learn more about uv."
    )


@pytest.fixture()
def spark() -> SparkSession:
    """Provide a SparkSession fixture for tests."""
    return DatabricksSession.builder.getOrCreate()


@pytest.fixture()
def load_fixture(spark: SparkSession):
    """Provide a callable to load JSON or CSV from fixtures/ directory."""

    def _loader(filename: str):
        path = pathlib.Path(__file__).parent.parent / "fixtures" / filename
        suffix = path.suffix.lower()
        if suffix == ".json":
            rows = json.loads(path.read_text())
            return spark.createDataFrame(rows)
        if suffix == ".csv":
            with path.open(newline="") as f:
                rows = list(csv.DictReader(f))
            return spark.createDataFrame(rows)
        raise ValueError(f"Unsupported fixture type for: {filename}")

    return _loader


def _enable_fallback_compute():
    """Enable serverless compute if no compute is specified."""
    conf = WorkspaceClient().config
    if conf.serverless_compute_id or conf.cluster_id or os.environ.get("SPARK_REMOTE"):
        return

    url = "https://docs.databricks.com/dev-tools/databricks-connect/cluster-config"
    print("no compute specified, falling back to serverless compute", file=sys.stderr)
    print(f"  see {url} for manual configuration", file=sys.stdout)

    os.environ["DATABRICKS_SERVERLESS_COMPUTE_ID"] = "auto"


@contextmanager
def _allow_stderr_output(config):
    """Temporarily disable pytest output capture."""
    capman = config.pluginmanager.get_plugin("capturemanager")
    if capman:
        with capman.global_and_fixture_disabled():
            yield
    else:
        yield


def pytest_configure(config):
    """Configure pytest session."""
    with _allow_stderr_output(config):
        _enable_fallback_compute()

        if hasattr(DatabricksSession.builder, "validateSession"):
            DatabricksSession.builder.validateSession().getOrCreate()
        else:
            DatabricksSession.builder.getOrCreate()
```

**Step 3: Create `fixtures/.gitkeep`**

Empty file.

**Step 4: Commit**

```bash
git add tests/__init__.py tests/conftest.py fixtures/.gitkeep
git commit -m "feat: add test configuration and fixtures directory"
```

---

## Task 10: Tests for Pydantic Models

**Files:**
- Create: `tests/test_models.py`

**Step 1: Write the failing tests**

These tests validate the Pydantic models locally (no Databricks connection needed).

```python
"""Tests for resource generation Pydantic models."""

import pytest
from scripts.models import PipelineResourceConfig, JobResourceConfig


class TestPipelineResourceConfig:
    def test_valid_config(self):
        config = PipelineResourceConfig(domain="finance", source="stripe", name="sync")
        assert config.resource_var_name == "finance_stripe_sync_pipeline"
        assert config.display_name == "finance_stripe_sync_pipeline"
        assert config.root_path == "src/finance/stripe"
        assert config.serverless is True

    def test_rejects_non_snake_case_domain(self):
        with pytest.raises(ValueError, match="must be snake_case"):
            PipelineResourceConfig(domain="Finance", source="stripe", name="sync")

    def test_rejects_non_snake_case_source(self):
        with pytest.raises(ValueError, match="must be snake_case"):
            PipelineResourceConfig(domain="finance", source="Stripe", name="sync")

    def test_rejects_non_snake_case_name(self):
        with pytest.raises(ValueError, match="must be snake_case"):
            PipelineResourceConfig(domain="finance", source="stripe", name="my-sync")

    def test_rejects_name_starting_with_number(self):
        with pytest.raises(ValueError, match="must be snake_case"):
            PipelineResourceConfig(domain="finance", source="stripe", name="1sync")


class TestJobResourceConfig:
    def test_valid_config(self):
        config = JobResourceConfig(domain="finance", source="stripe", name="sync")
        assert config.resource_var_name == "finance_stripe_sync_job"
        assert config.display_name == "finance_stripe_sync_job"
        assert config.pipeline_resource_var == "finance_stripe_sync_pipeline"
        assert config.schedule_interval == 1
        assert config.schedule_unit == "DAYS"

    def test_custom_schedule(self):
        config = JobResourceConfig(
            domain="finance", source="stripe", name="sync",
            schedule_interval=6, schedule_unit="HOURS",
        )
        assert config.schedule_interval == 6
        assert config.schedule_unit == "HOURS"

    def test_rejects_invalid_schedule_unit(self):
        with pytest.raises(ValueError, match="schedule_unit must be one of"):
            JobResourceConfig(
                domain="finance", source="stripe", name="sync",
                schedule_unit="MINUTES",
            )

    def test_rejects_non_snake_case(self):
        with pytest.raises(ValueError, match="must be snake_case"):
            JobResourceConfig(domain="Finance", source="stripe", name="sync")
```

**Step 2: Run tests to verify they fail**

Run: `uv run pytest tests/test_models.py -v`
Expected: FAIL — `scripts.models` does not exist yet (if running before Task 5) or PASS (if running after Task 5).

Note: If executing in order, this should PASS since Task 5 creates the models first.

**Step 3: Run tests to verify they pass**

Run: `uv run pytest tests/test_models.py -v`
Expected: All 9 tests PASS

**Step 4: Commit**

```bash
git add tests/test_models.py
git commit -m "test: add unit tests for Pydantic resource generation models"
```

---

## Task 11: Tests for Generator Scripts

**Files:**
- Create: `tests/test_generators.py`

**Step 1: Write the tests**

These tests run the generator scripts and verify the output files.

```python
"""Tests for resource generator scripts."""

import subprocess
import sys
from pathlib import Path

import pytest


@pytest.fixture(autouse=True)
def clean_output(tmp_path, monkeypatch):
    """Run generators in a temp directory to avoid polluting the repo."""
    monkeypatch.chdir(tmp_path)
    (tmp_path / "resources").mkdir()
    yield tmp_path


def run_script(module: str, *args: str) -> subprocess.CompletedProcess:
    return subprocess.run(
        [sys.executable, "-m", module, *args],
        capture_output=True,
        text=True,
    )


class TestBuildPipelineResource:
    def test_creates_pipeline_file(self, clean_output):
        result = run_script(
            "scripts.build_pipeline_resource",
            "--domain", "finance",
            "--source", "stripe",
            "--name", "sync",
        )
        assert result.returncode == 0
        output_file = clean_output / "resources" / "finance" / "stripe" / "sync_pipeline.py"
        assert output_file.exists()
        content = output_file.read_text()
        assert "Pipeline.from_dict" in content
        assert "finance_stripe_sync_pipeline" in content
        assert "${var.catalog}" in content
        assert "${var.schema}" in content

    def test_rejects_invalid_domain(self):
        result = run_script(
            "scripts.build_pipeline_resource",
            "--domain", "Finance",
            "--source", "stripe",
            "--name", "sync",
        )
        assert result.returncode != 0
        assert "Validation error" in result.stderr

    def test_rejects_duplicate_file(self, clean_output):
        run_script(
            "scripts.build_pipeline_resource",
            "--domain", "finance",
            "--source", "stripe",
            "--name", "sync",
        )
        result = run_script(
            "scripts.build_pipeline_resource",
            "--domain", "finance",
            "--source", "stripe",
            "--name", "sync",
        )
        assert result.returncode != 0
        assert "already exists" in result.stderr


class TestBuildJobResource:
    def test_creates_job_file(self, clean_output):
        result = run_script(
            "scripts.build_job_resource",
            "--domain", "finance",
            "--source", "stripe",
            "--name", "sync",
        )
        assert result.returncode == 0
        output_file = clean_output / "resources" / "finance" / "stripe" / "sync_job.py"
        assert output_file.exists()
        content = output_file.read_text()
        assert "Job.from_dict" in content
        assert "finance_stripe_sync_job" in content
        assert "finance_stripe_sync_pipeline" in content
        assert "${var.catalog}" in content

    def test_custom_schedule(self, clean_output):
        result = run_script(
            "scripts.build_job_resource",
            "--domain", "finance",
            "--source", "stripe",
            "--name", "sync",
            "--schedule-interval", "6",
            "--schedule-unit", "HOURS",
        )
        assert result.returncode == 0
        content = (clean_output / "resources" / "finance" / "stripe" / "sync_job.py").read_text()
        assert '"interval": 6' in content
        assert '"unit": "HOURS"' in content

    def test_rejects_invalid_schedule_unit(self):
        result = run_script(
            "scripts.build_job_resource",
            "--domain", "finance",
            "--source", "stripe",
            "--name", "sync",
            "--schedule-unit", "MINUTES",
        )
        assert result.returncode != 0
        assert "Validation error" in result.stderr
```

**Step 2: Run tests**

Run: `uv run pytest tests/test_generators.py -v`
Expected: All 7 tests PASS

**Step 3: Commit**

```bash
git add tests/test_generators.py
git commit -m "test: add integration tests for resource generator scripts"
```

---

## Task 12: Scaffold First Domain/Source (finance/stripe)

This task uses the `just scaffold` command to create the directory structure, then uses the generators to create the first pipeline and job resources.

**Files (all created by commands):**
- `src/finance/__init__.py`
- `src/finance/stripe/__init__.py`
- `src/finance/stripe/transformations/.gitkeep`
- `resources/finance/__init__.py`
- `resources/finance/stripe/__init__.py`
- `tests/finance/__init__.py`
- `tests/finance/stripe/__init__.py`
- `fixtures/finance/stripe/.gitkeep`

**Step 1: Run scaffold**

Run: `just scaffold finance stripe`
Expected: "Scaffolded finance/stripe"

**Step 2: Verify directory structure**

Run: `find src/finance resources/finance tests/finance fixtures/finance -type f | sort`
Expected:
```
fixtures/finance/stripe/.gitkeep
resources/finance/__init__.py
resources/finance/stripe/__init__.py
src/finance/__init__.py
src/finance/stripe/__init__.py
src/finance/stripe/transformations/.gitkeep
tests/finance/__init__.py
tests/finance/stripe/__init__.py
```

**Step 3: Generate pipeline resource**

Run: `just build-pipeline-resource finance stripe sync`
Expected: "Created resources/finance/stripe/sync_pipeline.py"

**Step 4: Generate job resource**

Run: `just build-job-resource finance stripe sync`
Expected: "Created resources/finance/stripe/sync_job.py"

**Step 5: Verify generated files contain correct content**

Run: `head -5 resources/finance/stripe/sync_pipeline.py`
Expected: Contains `from databricks.bundles.pipelines import Pipeline`

Run: `head -5 resources/finance/stripe/sync_job.py`
Expected: Contains `from databricks.bundles.jobs import Job`

**Step 6: Commit**

```bash
git add src/finance/ resources/finance/ tests/finance/ fixtures/finance/
git commit -m "feat: scaffold finance/stripe domain with pipeline and job resources"
```

---

## Task 13: Sample DLT Transformations for finance/stripe

Create sample DLT transformation files in the four layers (raw, base, staging, final) to demonstrate the pattern.

**Files:**
- Create: `src/finance/stripe/transformations/raw_payments.py`
- Create: `src/finance/stripe/transformations/base_payments.py`
- Create: `src/finance/stripe/transformations/staging_payments.py`
- Create: `src/finance/stripe/transformations/final_payments.py`
- Remove: `src/finance/stripe/transformations/.gitkeep`

**Step 1: Create `src/finance/stripe/transformations/raw_payments.py`**

```python
"""Raw layer: ingest raw payment data from source."""

from pyspark import pipelines as dp


@dp.table
def raw_payments():
    """Raw payment events from Stripe ingestion."""
    # TODO: replace with actual ingestion source
    return spark.read.table("raw.stripe.payments")
```

**Step 2: Create `src/finance/stripe/transformations/base_payments.py`**

```python
"""Base layer: cast data types and deduplicate."""

from pyspark import pipelines as dp
from pyspark.sql.functions import col, to_timestamp
from pyspark.sql.types import DecimalType


@dp.table
def base_payments():
    """Payments with correct data types and deduplication."""
    return (
        spark.read.table("raw_payments")
        .dropDuplicates(["payment_id"])
        .select(
            col("payment_id").cast("string"),
            col("amount").cast(DecimalType(18, 2)),
            col("currency").cast("string"),
            col("status").cast("string"),
            to_timestamp(col("created_at")).alias("created_at"),
        )
    )
```

**Step 3: Create `src/finance/stripe/transformations/staging_payments.py`**

```python
"""Staging layer: joins and business logic."""

from pyspark import pipelines as dp
from pyspark.sql.functions import col, when


@dp.table
def staging_payments():
    """Payments enriched with business logic."""
    return (
        spark.read.table("base_payments")
        .withColumn(
            "is_successful",
            when(col("status") == "succeeded", True).otherwise(False),
        )
    )
```

**Step 4: Create `src/finance/stripe/transformations/final_payments.py`**

```python
"""Final layer: reporting aggregations."""

from pyspark import pipelines as dp
from pyspark.sql.functions import col, sum, count


@dp.table
def final_payment_summary():
    """Payment summary aggregations for reporting."""
    return (
        spark.read.table("staging_payments")
        .filter(col("is_successful"))
        .groupBy(col("currency"))
        .agg(
            sum("amount").alias("total_amount"),
            count("payment_id").alias("transaction_count"),
        )
    )
```

**Step 5: Remove the .gitkeep since transformations now exist**

Run: `rm src/finance/stripe/transformations/.gitkeep`

**Step 6: Commit**

```bash
git add src/finance/stripe/transformations/
git commit -m "feat: add sample DLT transformations for finance/stripe (raw, base, staging, final)"
```

---

## Task 14: Update README

**Files:**
- Modify: `README.md`

**Step 1: Update `README.md`**

```markdown
# data-platform

Databricks Asset Bundles monorepo for the data platform.

## Structure

```
databricks.yml              # Root bundle configuration
resources/                   # Python-defined bundle resources (jobs, pipelines)
  <domain>/<source>/         # Resources scoped per domain/source
src/                         # Source code
  <domain>/<source>/         # Domain/source packages
    transformations/         # DLT transformation files (raw, base, staging, final)
scripts/                     # Resource generation tooling
tests/                       # Tests
fixtures/                    # Test fixtures (JSON, CSV)
```

## Namespace Mapping

- **catalog** = domain (e.g., `finance`)
- **schema** = source (e.g., `stripe`)

## Data Layers

| Layer | Purpose |
|-------|---------|
| `raw` | Raw ingestion from source |
| `base` | Cast data types and deduplicate |
| `staging` | Joins and business logic |
| `final` | Reporting aggregations |

## Quick Start

```bash
# Scaffold a new domain/source
just scaffold <domain> <source>

# Generate a pipeline resource
just build-pipeline-resource <domain> <source> <name>

# Generate a job resource
just build-job-resource <domain> <source> <name>

# Validate bundle
just validate

# Deploy to dev
just deploy

# Run tests
just test
```

## Targets

| Target | Mode | Description |
|--------|------|-------------|
| `dev` | development | Prefixed resources, paused schedules |
| `staging` | default | Shared staging workspace |
| `prod` | production | Strict checks, managed permissions |
```

**Step 2: Commit**

```bash
git add README.md
git commit -m "docs: update README with project structure and usage"
```

---

## Summary

| Task | Description | Files |
|------|-------------|-------|
| 1 | Root bundle configuration | `databricks.yml` |
| 2 | Python project configuration | `pyproject.toml` |
| 3 | VS Code configuration | `.vscode/settings.json`, `.vscode/extensions.json`, `.gitignore` |
| 4 | Resource loader entry point | `resources/__init__.py` |
| 5 | Pydantic models for validation | `scripts/__init__.py`, `scripts/models.py` |
| 6 | Pipeline resource generator | `scripts/build_pipeline_resource.py` |
| 7 | Job resource generator | `scripts/build_job_resource.py` |
| 8 | Justfile with commands | `justfile` |
| 9 | Test configuration | `tests/__init__.py`, `tests/conftest.py`, `fixtures/.gitkeep` |
| 10 | Tests for Pydantic models | `tests/test_models.py` |
| 11 | Tests for generator scripts | `tests/test_generators.py` |
| 12 | Scaffold finance/stripe | Directory structure + generated resources |
| 13 | Sample DLT transformations | 4 transformation files (raw, base, staging, final) |
| 14 | Update README | `README.md` |

## Suggestions for Future Improvements

- CI/CD pipeline with domain-scoped deployment blast radius
- `run_as` with service principals per target
- Shared utility modules in `src/common/` or `lib/`
- Schema evolution and data quality expectations (`@dp.expect`)
- Pre-commit hooks for ruff formatting and YAML validation
- Databricks CLI profile management per environment
