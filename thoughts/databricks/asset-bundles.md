# Databricks Asset Bundles Plan

## Summary
Build a single-root Databricks Asset Bundle monorepo that scales across many domains/sources. Bundle resources follow the Databricks `pydabs` template pattern located in `/tmp/databricks-pydabs-template`: minimal root YAML plus Python-defined resources loaded from `resources`. Keep structural mapping stable as `catalog = domain` and `schema = source`.

It's acceptable to add placeholders with .gitkeep and provide an `__init__` on what the folder is intended for.

## Architecture Decisions
- Repo model: monorepo, one root `databricks.yml`.
- Environment targets: `dev`, `staging`, `prod`.
- Workspace topology: one Databricks workspace per environment.
- Code transforms (pyspark, sparksql): `raw`, `base`, `staging`, `final`
    - `raw`: this is the raw ingestion layer
    - `base`: cast data types and dedupe data
    - `staging`: joins and business logic
    - `final`: reporting layer, aggreations
- Data namespace mapping:
  - `catalog = domain`
  - `schema = source`
- Asset packaging: each `domain/source` package contains both DLT and jobs.
- Deployment blast radius: domain-scoped by default in CI/CD.

## Research
- Use context7 libraryId /websites/databricks to research asset bundles to understand what goes in `resources` and `src`
- A databricks `pydabs` template exists in `/tmp/databricks-pydabs-template`

## Requirements
- Python-defined resources loaded from `resources` must be auto generated by the user running a just command `just build-job-resource` or `just build-pipeline-resource`
    - This is to foce type checking using PyDantic, strict formatting, and have both job and pipeline requirements defined

## Avoid
- Do not deviate from the databricks provided template too much. Instead save your suggestions/recommedations for future improvements/implementations.
- Do not make assumptions, prompt the user with any clarifications or questions
